{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqvY94XBQqkKpM9BlwHkXT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1jv2QfudWR5U","executionInfo":{"status":"ok","timestamp":1686153167760,"user_tz":-480,"elapsed":2875,"user":{"displayName":"Min Han Chung","userId":"17612809496234870196"}},"outputId":"945d5cc0-e46d-482a-c8b3-41241f383a90"},"outputs":[{"output_type":"stream","name":"stdout","text":["iter 0 result 38\n","iter 100 result 18\n","iter 200 result 18\n","iter 300 result 18\n","iter 400 result 18\n","iter 500 result 18\n","iter 600 result 18\n","iter 700 result 18\n","iter 800 result 18\n","iter 900 result 18\n","index 0 movement 1\n","index 1 movement 1\n","index 2 movement 2\n","index 3 movement 2\n","index 4 movement 2\n","index 5 movement 2\n","index 6 movement 2\n","index 7 movement 1\n","index 8 movement 1\n","index 9 movement 1\n","index 10 movement 1\n","index 11 movement 2\n","index 12 movement 2\n","index 13 movement 2\n","index 14 movement 1\n","index 15 movement 2\n","index 16 movement 1\n","index 17 movement 1\n"]}],"source":["#problem1 a\n","# this code is used to generate a trajectory using monte carlo or probability emthod\n","# with 2 trajectory storage capacity\n","# we first generate a random movement and check if the movement is legal by checking\n","# if it hits the wall or obstacle\n","# if it pass the test, we execute it by changing state variable\n","\n","import random\n","\n","width=10\n","length=10\n","\n","def insidewindow(x,y):\n","    if x<width and x>=0:\n","        if y<length and y>=0:\n","            return True\n","    else:\n","        return False\n","def obstacle(x,y):\n","    if x==0 and y==3: \n","        return False # hitting the obstacle \n","    elif x==1 and y==3:\n","        return False\n","    elif x==2 and y==3:\n","        return False\n","    elif x==3 and y==3:\n","        return False\n","    elif x==4 and y==3:\n","        return False\n","    elif x==7 and y==3:\n","        return False\n","    elif x==8 and y==3:\n","        return False\n","    elif x==9 and y==3:\n","        return False\n","    else:\n","        return True\n","\n","prev_trajectory= [0,0,0,0,0,0,0,0,0,0,0, \\\n","                0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0]\n","\n","current_trajectory= [0,0,0,0,0,0,0,0,0,0,0, \\\n","                0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0]\n","\n","PI= [0.6,0.2,0.2] #probability of moving up, moving right, and moving left\n","\n","count_move=0  # counting of legal move\n","count_move_prev=100 # record of best move minimal no of move\n","for k in range(1000):\n","    state_x=0\n","    state_y=0\n","    count_move=0\n","    for l in range(100):\n","            current_trajectory[l]=0 #make all item to zero\n","    for i in range(100):  # the number 100 need to be fixed maximal number of trials\n","        p=random.uniform(0,1) #generate another random number\n","        if p>=0 and p<=PI[0]:\n","            state_y=state_y+1  # MOVE UP\n","            if insidewindow(state_x,state_y) and obstacle(state_x,state_y):\n","                current_trajectory[count_move]= 1\n","                count_move=count_move+1\n","            else:\n","                state_y=state_y-1\n","               \n","        elif p>PI[0] and p<=(PI[0]+PI[1]):\n","            state_x=state_x+1 #move right\n","            if insidewindow(state_x,state_y)and obstacle(state_x,state_y):\n","                current_trajectory[count_move]= 2\n","                count_move=count_move+1\n","            else:\n","                state_x=state_x-1\n","        else:\n","            state_x=state_x-1 #move left\n","            if insidewindow(state_x,state_y)and obstacle(state_x,state_y):\n","                current_trajectory[count_move]= 3\n","                count_move=count_move+1\n","            else:\n","                state_x=state_x+1\n","        #goal checking \n","        if state_x==width-1 and state_y==length-1:\n","            # unmask the following line to check if goal is reached\n","            #print('iteration',k, 'move', count_move, 'goal reached')\n","            break\n","    if count_move<count_move_prev: #if the current move is better\n","        count_move_prev=count_move\n","        for j in range(100):\n","            prev_trajectory[j]=current_trajectory[j] #store the move history\n","    if k % 100==0:  #sampling result of improvement when 10000 iteration used change 100 to 1000\n","        \n","        print('iter', k, 'result', count_move_prev)\n","        \n","for i in range(100):\n","    if prev_trajectory[i] !=0:\n","        \n","        print('index', i, 'movement', prev_trajectory[i])        \n"]},{"cell_type":"code","source":["#problem1 b\n","# this code is used to generate a trajectory using monte carlo or probability emthod\n","# with 2 trajectory storage capacity\n","# we first generate a random movement and check if the movement is legal by checking\n","# if it hits the wall or obstacle\n","# if it pass the test, we execute it by changing state variable\n","\n","import random\n","\n","width=10\n","length=10\n","\n","def insidewindow(x,y):\n","    if x<width and x>=0:\n","        if y<length and y>=0:\n","            return True\n","    else:\n","        return False\n","def obstacle(x,y):\n","    if x==5 and y==0: \n","        return False # hitting the obstacle \n","    elif x==5 and y==1:\n","        return False\n","    elif x==5 and y==2:\n","        return False\n","    elif x==5 and y==3:\n","        return False\n","    elif x==5 and y==6:\n","        return False\n","    elif x==5 and y==7:\n","        return False\n","    elif x==5 and y==8:\n","        return False\n","    elif x==5 and y==9:\n","        return False\n","    else:\n","        return True\n","prev_trajectory= [0,0,0,0,0,0,0,0,0,0,0, \\\n","                0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0]\n","\n","current_trajectory= [0,0,0,0,0,0,0,0,0,0,0, \\\n","                0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0,\\\n","                     0,0,0,0,0,0,0,0,0,0,0]\n","\n","PI= [0.6,0.2,0.2] #probability of moving up, moving right, and moving left\n","\n","count_move=0  # counting of legal move\n","count_move_prev=100 # record of best move minimal no of move\n","for k in range(1000):\n","    state_x=0\n","    state_y=0\n","    count_move=0\n","    for l in range(100):\n","            current_trajectory[l]=0 #make all item to zero\n","    for i in range(100):  # the number 100 need to be fixed maximal number of trials\n","        p=random.uniform(0,1) #generate another random number\n","        if p>=0 and p<=PI[0]:\n","            state_y=state_y+1  # MOVE UP\n","            if insidewindow(state_x,state_y) and obstacle(state_x,state_y):\n","                current_trajectory[count_move]= 1\n","                count_move=count_move+1\n","            else:\n","                state_y=state_y-1\n","               \n","        elif p>PI[0] and p<=(PI[0]+PI[1]):\n","            state_x=state_x+1 #move right\n","            if insidewindow(state_x,state_y)and obstacle(state_x,state_y):\n","                current_trajectory[count_move]= 2\n","                count_move=count_move+1\n","            else:\n","                state_x=state_x-1\n","        else:\n","            state_y=state_y-1 #move down\n","            if insidewindow(state_x,state_y)and obstacle(state_x,state_y):\n","                current_trajectory[count_move]= 3\n","                count_move=count_move+1\n","            else:\n","                state_y=state_y+1\n","        #goal checking \n","        if state_x==9 and state_y==0:\n","            # unmask the following line to check if goal is reached\n","            print('iteration',k, 'move', count_move, 'goal reached')\n","            break\n","    if count_move<count_move_prev: #if the current move is better\n","        count_move_prev=count_move\n","        for j in range(100):\n","            prev_trajectory[j]=current_trajectory[j] #store the move history\n","    if k % 100==0:  #sampling result of improvement when 10000 iteration used change 100 to 1000\n","        \n","        print('iter', k, 'result', count_move_prev)\n","        \n","for i in range(100):\n","    if prev_trajectory[i] !=0:\n","        \n","        print('index', i, 'movement', prev_trajectory[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B50oEKrOeFT9","executionInfo":{"status":"ok","timestamp":1686154846628,"user_tz":-480,"elapsed":677,"user":{"displayName":"Min Han Chung","userId":"17612809496234870196"}},"outputId":"99af0cdd-5446-492d-bc2b-6df74adbc2c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iter 0 result 59\n","iter 100 result 31\n","iter 200 result 31\n","iter 300 result 31\n","iter 400 result 31\n","iteration 455 move 33 goal reached\n","iteration 478 move 29 goal reached\n","iter 500 result 29\n","iter 600 result 29\n","iter 700 result 29\n","iteration 753 move 59 goal reached\n","iter 800 result 29\n","iter 900 result 29\n","index 0 movement 1\n","index 1 movement 3\n","index 2 movement 2\n","index 3 movement 2\n","index 4 movement 1\n","index 5 movement 3\n","index 6 movement 1\n","index 7 movement 3\n","index 8 movement 2\n","index 9 movement 1\n","index 10 movement 3\n","index 11 movement 1\n","index 12 movement 2\n","index 13 movement 1\n","index 14 movement 1\n","index 15 movement 1\n","index 16 movement 2\n","index 17 movement 1\n","index 18 movement 3\n","index 19 movement 1\n","index 20 movement 2\n","index 21 movement 2\n","index 22 movement 2\n","index 23 movement 2\n","index 24 movement 3\n","index 25 movement 3\n","index 26 movement 3\n","index 27 movement 3\n","index 28 movement 3\n"]}]},{"cell_type":"code","source":["#problem 2\n","# Commented out IPython magic to ensure Python compatibility.\n","# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# TensorFlow ≥2.0 is required\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","\n","\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","# %matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","np.random.seed(42)\n","\n","transition_probabilities = [ # shape=[s, s']\n","        [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n","        [0.0, 1.0, 0.0, 0.0],  # from s1 to ...\n","        [0.0, 0.9, 0.0, 0.1],  # from s2 to ...\n","        [0.8, 0.2, 0.0, 0.0]]  # from s3 to ...\n","\n","n_max_steps = 50\n","\n","def print_sequence():\n","    current_state = 0\n","    print(\"States:\", end=\" \")\n","    for step in range(n_max_steps):\n","        print(current_state, end=\" \")\n","        # unmask the following and change the 3 to the expected terminal state\n","        if current_state == 3:\n","            break\n","        current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n","    else:\n","        print(\"...\", end=\"\")\n","    print()\n","\n","for _ in range(10):\n","    print_sequence()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmrZ5Z7NWai1","executionInfo":{"status":"ok","timestamp":1686155117577,"user_tz":-480,"elapsed":3108,"user":{"displayName":"Min Han Chung","userId":"17612809496234870196"}},"outputId":"094a8fdc-30eb-4b02-bd48-2dcb95c35278"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["States: 0 0 3 \n","States: 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n","States: 0 3 \n","States: 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n","States: 0 0 3 \n","States: 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n","States: 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n","States: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...\n","States: 0 0 3 \n","States: 0 0 0 0 0 3 \n"]}]},{"cell_type":"code","source":["#Problem 3 a\n","# Commented out IPython magic to ensure Python compatibility.\n","# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# TensorFlow ≥2.0 is required\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","\n","\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","# %matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","np.random.seed(42)\n","\n","\n","# markov decision process \n","transition_probabilities = [ # shape=[s, a, s']\n","        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n","        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n","        [None, [0.8, 0.1, 0.1], None]]\n","rewards = [ # shape=[s, a, s']\n","        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n","        [[0, 0, 0], [0, 0, 0], [0, 0, +50]],\n","        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n","possible_actions = [[0, 1, 2], [0, 2], [1]]\n","\n","Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n","for state, actions in enumerate(possible_actions):\n","    Q_values[state, actions] = 0.0  # for all possible actions\n","\n","gamma = 0.9  # the discount factor\n","\n","history1 = [] # Not shown in the book (for the figure below)\n","for iteration in range(50):\n","    Q_prev = Q_values.copy()\n","    history1.append(Q_prev) # Not shown\n","    for s in range(3):\n","        for a in possible_actions[s]:\n","            Q_values[s, a] = np.sum([\n","                    transition_probabilities[s][a][sp]\n","                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n","                for sp in range(3)])\n","\n","history1 = np.array(history1) # Not shown\n","Q_values\n","#np.argmax(Q_values, axis=1)\n","# it is best to check Q_values and then optimal policy\n","#The optimal policy for this MDP, when using a discount factor of 0.90, \n","#is to choose action a0 when in state s0, and choose action a0 when in state s1,\n","# and finally choose action a1 (the only possible action) when in state s2.\n"],"metadata":{"id":"WjVcNkhKWlaO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686160397442,"user_tz":-480,"elapsed":468,"user":{"displayName":"Min Han Chung","userId":"17612809496234870196"}},"outputId":"17b6f705-d9f0-44fc-a691-da1edb65b9d2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[195.20173993, 175.57359999, 183.99235995],\n","       [217.66739979,         -inf, 241.9726286 ],\n","       [        -inf, 213.42288283,         -inf]])"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["#Problem 3 b\n","# Commented out IPython magic to ensure Python compatibility.\n","# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# TensorFlow ≥2.0 is required\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","\n","\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","# %matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","np.random.seed(42)\n","\n","\n","# markov decision process \n","transition_probabilities = [ # shape=[s, a, s']\n","        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n","        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n","        [None, [0.8, 0.1, 0.1], None]]\n","rewards = [ # shape=[s, a, s']\n","        [[+10, +40, 0], [0, 0, 0], [0, 0, 0]],\n","        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n","        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n","possible_actions = [[0, 1, 2], [0, 2], [1]]\n","\n","Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n","for state, actions in enumerate(possible_actions):\n","    Q_values[state, actions] = 0.0  # for all possible actions\n","\n","gamma = 0.9  # the discount factor\n","\n","history1 = [] # Not shown in the book (for the figure below)\n","for iteration in range(50):\n","    Q_prev = Q_values.copy()\n","    history1.append(Q_prev) # Not shown\n","    for s in range(3):\n","        for a in possible_actions[s]:\n","            Q_values[s, a] = np.sum([\n","                    transition_probabilities[s][a][sp]\n","                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n","                for sp in range(3)])\n","\n","history1 = np.array(history1) # Not shown\n","Q_values\n","#np.argmax(Q_values, axis=1)\n","# it is best to check Q_values and then optimal policy\n","#The optimal policy for this MDP, when using a discount factor of 0.90, \n","#is to choose action a0 when in state s0, and choose action a0 when in state s1,\n","# and finally choose action a1 (the only possible action) when in state s2."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wpdhgxFr-yLp","executionInfo":{"status":"ok","timestamp":1686160401068,"user_tz":-480,"elapsed":495,"user":{"displayName":"Min Han Chung","userId":"17612809496234870196"}},"outputId":"6bd4bcb9-d44f-4293-a085-3ba2c85a2776"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 84.98201404,  76.45048182,  69.4715033 ],\n","       [ 41.55558921,         -inf,  46.20991113],\n","       [        -inf, 106.93693549,         -inf]])"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["#Problem 3 c\n","# Commented out IPython magic to ensure Python compatibility.\n","# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# TensorFlow ≥2.0 is required\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","\n","\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","# %matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","np.random.seed(42)\n","\n","\n","# markov decision process \n","transition_probabilities = [ # shape=[s, a, s']\n","        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n","        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n","        [None, [0.8, 0.1, 0.1], None]]\n","rewards = [ # shape=[s, a, s']\n","        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n","        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n","        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n","possible_actions = [[0, 1, 2], [0, 2], [1]]\n","\n","Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n","for state, actions in enumerate(possible_actions):\n","    Q_values[state, actions] = 0.0  # for all possible actions\n","\n","gamma = 0.8  # the discount factor\n","\n","history1 = [] # Not shown in the book (for the figure below)\n","for iteration in range(50):\n","    Q_prev = Q_values.copy()\n","    history1.append(Q_prev) # Not shown\n","    for s in range(3):\n","        for a in possible_actions[s]:\n","            Q_values[s, a] = np.sum([\n","                    transition_probabilities[s][a][sp]\n","                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n","                for sp in range(3)])\n","\n","history1 = np.array(history1) # Not shown\n","Q_values\n","#np.argmax(Q_values, axis=1)\n","# it is best to check Q_values and then optimal policy\n","#The optimal policy for this MDP, when using a discount factor of 0.90, \n","#is to choose action a0 when in state s0, and choose action a0 when in state s1,\n","# and finally choose action a1 (the only possible action) when in state s2.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgsDditdPPu7","executionInfo":{"status":"ok","timestamp":1686198569444,"user_tz":-480,"elapsed":318,"user":{"displayName":"Min Han Chung","userId":"17612809496234870196"}},"outputId":"e10fd285-e32a-4424-e96d-41acd350765a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 15.90909091,  12.72727273,  10.18181818],\n","       [  0.        ,         -inf, -13.3201581 ],\n","       [        -inf,  45.84980237,         -inf]])"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["#Problem 3 c\n","# Commented out IPython magic to ensure Python compatibility.\n","# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# TensorFlow ≥2.0 is required\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","\n","\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","# %matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","np.random.seed(42)\n","\n","\n","# markov decision process \n","transition_probabilities = [ # shape=[s, a, s']\n","        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n","        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n","        [None, [0.8, 0.1, 0.1], None]]\n","rewards = [ # shape=[s, a, s']\n","        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n","        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n","        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n","possible_actions = [[0, 1, 2], [0, 2], [1]]\n","\n","Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n","for state, actions in enumerate(possible_actions):\n","    Q_values[state, actions] = 0.0  # for all possible actions\n","\n","gamma = 0.95  # the discount factor\n","\n","history1 = [] # Not shown in the book (for the figure below)\n","for iteration in range(50):\n","    Q_prev = Q_values.copy()\n","    history1.append(Q_prev) # Not shown\n","    for s in range(3):\n","        for a in possible_actions[s]:\n","            Q_values[s, a] = np.sum([\n","                    transition_probabilities[s][a][sp]\n","                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n","                for sp in range(3)])\n","\n","history1 = np.array(history1) # Not shown\n","Q_values\n","#np.argmax(Q_values, axis=1)\n","# it is best to check Q_values and then optimal policy\n","#The optimal policy for this MDP, when using a discount factor of 0.90, \n","#is to choose action a0 when in state s0, and choose action a0 when in state s1,\n","# and finally choose action a1 (the only possible action) when in state s2.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0Os_0fAP0W2","executionInfo":{"status":"ok","timestamp":1686198257082,"user_tz":-480,"elapsed":5,"user":{"displayName":"Min Han Chung","userId":"17612809496234870196"}},"outputId":"1c9b0f1b-714e-48fa-a914-27187495e1cb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[21.73304188, 20.63807938, 16.70138772],\n","       [ 0.95462106,        -inf,  1.01361207],\n","       [       -inf, 53.70728682,        -inf]])"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# Commented out IPython magic to ensure Python compatibility.\n","# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# TensorFlow ≥2.0 is required\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","\n","\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","# %matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","np.random.seed(42)\n","\n","\n","# markov decision process \n","transition_probabilities = [ # shape=[s, a, s']\n","        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n","        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n","        [None, [0.8, 0.1, 0.1], None]]\n","rewards = [ # shape=[s, a, s']\n","        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n","        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n","        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n","possible_actions = [[0, 1, 2], [0, 2], [1]]\n","\n","Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n","for state, actions in enumerate(possible_actions):\n","    Q_values[state, actions] = 0.0  # for all possible actions\n","\n","gamma = 0.90  # the discount factor\n","\n","history1 = [] # Not shown in the book (for the figure below)\n","for iteration in range(50):\n","    Q_prev = Q_values.copy()\n","    history1.append(Q_prev) # Not shown\n","    for s in range(3):\n","        for a in possible_actions[s]:\n","            Q_values[s, a] = np.sum([\n","                    transition_probabilities[s][a][sp]\n","                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n","                for sp in range(3)])\n","\n","history1 = np.array(history1) # Not shown\n","Q_values\n","#np.argmax(Q_values, axis=1)\n","# it is best to check Q_values and then optimal policy\n","#The optimal policy for this MDP, when using a discount factor of 0.90, \n","#is to choose action a0 when in state s0, and choose action a0 when in state s1,\n","# and finally choose action a1 (the only possible action) when in state s2.\n","\n","#Q learning \n","#We will need to simulate an agent moving around in the environment, \n","#so let's define a function to perform some action and get the new state \n","#and a reward:\n","def step(state, action):\n","    probas = transition_probabilities[state][action]\n","    next_state = np.random.choice([0, 1, 2], p=probas)\n","    reward = rewards[state][action][next_state]\n","    return next_state, reward\n","#We also need an exploration policy. We will just use a random policy, \n","# since the state space is very small:\n","def exploration_policy(state):\n","    return np.random.choice(possible_actions[state])\n","\n","np.random.seed(42)\n","\n","Q_values = np.full((3, 3), -np.inf)\n","for state, actions in enumerate(possible_actions):\n","    Q_values[state][actions] = 0\n","\n","alpha0 = 0.05 # initial learning rate\n","decay = 0.005 # learning rate decay\n","gamma = 0.90 # discount factor\n","state = 0 # initial state\n","history2 = [] # Not shown \n","\n","for iteration in range(10000):\n","    history2.append(Q_values.copy()) # Not shown\n","    action = exploration_policy(state)\n","    next_state, reward = step(state, action)\n","    next_value = np.max(Q_values[next_state]) # greedy policy at the next step\n","    alpha = alpha0 / (1 + iteration * decay)\n","    Q_values[state, action] *= 1 - alpha\n","    Q_values[state, action] += alpha * (reward + gamma * next_value)\n","    state = next_state\n","\n","history2 = np.array(history2) # Not shown\n","Q_values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h8kZeHC-_jwa","executionInfo":{"status":"ok","timestamp":1686596669099,"user_tz":-480,"elapsed":6968,"user":{"displayName":"Min Han Chung","userId":"17612809496234870196"}},"outputId":"fb0d5383-5ca7-414a-a698-49f16c24977f"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[18.77621289, 17.2238872 , 13.74543343],\n","       [ 0.        ,        -inf, -8.00485647],\n","       [       -inf, 49.40208921,        -inf]])"]},"metadata":{},"execution_count":1}]}]}